<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Chatbot</title>
  <link rel="stylesheet" href="bot_style.css">
</head>
<body>
  <div class="chatbot-container">
    <div class="chatbot-header">
      <div class="chatbot-title">Grace</div>
      <div class="chatbot-listening-controls">
        <label class="switch">
          <input type="checkbox" id="listen-checkbox" onchange="toggleListening()">
          <span class="slider"></span>
        </label>
        <span id="listening-status" class="listening-status">Inactive</span>
      </div>
    </div>

    <div class="chatbot-messages" id="messages"></div>

    <div class="chatbot-input">
      <input type="text" id="chat-input" placeholder="Type a message...">
      <button onclick="sendMessage()">Send</button>
    </div>
  </div>

  <script>
    let chatHistory = [];
    let intent = null;
    let lastSelector = null;
    let botMessage = null;
    const currentPage = window.parent.location.pathname.split("/").pop();

    // Function to get the summary of the last user message based on the current page
    // This function is called when the chatbot detects a confirmation or success page.
    function getSummary() {
      console.log("getSummary called for page:", currentPage);

      const patterns = {
        "confirm_transfer.html": { match: "You plan to send", emoji: /^âœ…\s*/ },
        "success.html": { match: "You successfully transfered", emoji: /^ðŸŽ‰\s*/ }
      };

      const pattern = patterns[currentPage];
      if (!pattern) {
        console.log("No pattern defined for this page.");
        return "";
      }

      console.log("Using pattern:", pattern);

      // Look for the last user message that matches
      for (let i = chatHistory.length - 1; i >= 0; i--) {
        const m = chatHistory[i];
        console.log("Checking message:", m);
        if (m.role === "user" && m.content.includes(pattern.match)) {
          const cleanedText = m.content.replace(pattern.emoji, '');
          console.log("Found matching message:", cleanedText);
          return cleanedText;
        }
      }

      console.log("No matching message found.");
      return "";
    }

    // Append a message to the chatbox
    // This function is called when the user sends a message or the assistant responds.
    function appendMessage(role, text, suppressTTS = false) {
      const messages = document.getElementById("messages");
      const div = document.createElement("div");
      div.className = "message " + role;
      div.innerText = text;
      messages.appendChild(div);
      messages.scrollTop = messages.scrollHeight;

      // Speak if the message is from assistant and on the listening mode
      if (role === "assistant" && listening && !suppressTTS) {
        if (currentPage === "confirm_transfer.html") {
          console.log("Confirm transfer page detected, checking for summary");
          const userLog = getSummary();
          if (userLog) {
            speak(userLog);
          }
        }

        if (currentPage === "success.html") {
          const userLog = getSummary();
          if (userLog) {
            speak(userLog);
          }
        }

        speak(text);
      }
    }
  
    // Highlight the element in the parent window
    // This function is called when the chatbot receives a selector from the backend
    function highlight(selector, lastInstruction = "mark-complete") {
      if (!selector) return;
      console.log("lastSelector and selector", lastSelector, selector);

      // Dehighlight the previous selector if it's different
      if (lastSelector && lastSelector !== selector) {
        console.log("dehighlighting in chatbot", lastSelector);
        window.parent.postMessage({selector: lastSelector, instruction: lastInstruction}, "*");
      }

      console.log("[Chatbot] Sending highlight to parent:", selector);
      // Highlight the new selector
      window.parent.postMessage({selector, instruction: "highlight"}, "*");
      lastSelector = selector; // Update the last selector
    }
    
    // Log user action to the chat history and sessionStorage
    function logUserAction(text) {
      appendMessage("user", text);
      chatHistory.push({ role: "user", content: text });
      sessionStorage.setItem("chatHistory", JSON.stringify(chatHistory));
    }

    // Update substep flags for the "Transfer Someone" page
    // This function checks the parent form fields to determine if the user has selected an account and entered an amount.
    // It returns an object with flags that indicate the progress of the substep.
    // This is used to track the user's progress in the transfer process.
    function updateSubstepFlagsForTransferSomeone() {
      const substep_flags = {};

      try {
        const parentDoc = window.parent.document;
        const account = parentDoc.querySelector("#from-account");
        const amount = parentDoc.querySelector("#amount");

        if (account && account.value !== "instruction") {
          substep_flags.account_chosen = true;
        }

        if (amount && parseFloat(amount.value) > 0) {
          substep_flags.amount_entered = true;
        }
      } catch (e) {
        console.warn("Unable to access parent form fields:", e);
      }

      return substep_flags;
    }

    function toggleListening() {
      const isChecked = document.getElementById("listen-checkbox").checked;
      const statusLabel = document.getElementById("listening-status");

      if (isChecked && !listening) {
        recognition.start();
        listening = true;
        statusLabel.textContent = "Active";
        sessionStorage.setItem("listening", "true");

        // Show first assistant message
        const welcomeMessage = "Hi! I'm Grace. What do you want to complete today?";
        appendMessage("assistant", welcomeMessage);

        // âœ… Only resume if intent already exists
        if (intent) {
          console.log("Calling sendMessage for resuming");
          sendMessage(true);
        }

      } else if (!isChecked && listening) {
        recognition.stop();
        listening = false;
        statusLabel.textContent = "Inactive";
        sessionStorage.setItem("listening", "false");
      }
    }


    // OPTION1: TTS function using server-side API
    // async function speak(text) {
    //   try {
    //     const response = await fetch("/speak", {
    //       method: "POST",
    //       headers: {
    //         "Content-Type": "application/json"
    //       },
    //       body: JSON.stringify({ text })
    //     });

    //     const result = await response.json();
    //     if (result.status === "success") {
    //       console.log("Speech played:", result.text);
    //     } else {
    //       console.error("TTS Error:", result.reason);
    //     }
    //   } catch (error) {
    //     console.error("Failed to fetch /speak:", error);
    //   }
    // }

    // OPTION2: TTS function using Web Speech API
    let voicesReady = false;
    let isSpeaking = false;

    speechSynthesis.onvoiceschanged = () => {
      voicesReady = true;
      console.log("Voices loaded:", speechSynthesis.getVoices());
    };

    function speak(text) {
      if (!('speechSynthesis' in window)) return;
      if (!voicesReady) return;

      const utterance = new SpeechSynthesisUtterance(text);
      utterance.lang = 'en-US';

      const voices = speechSynthesis.getVoices();
      const voice = voices.find(v => v.lang === 'en-AU' && v.name.includes("Google"));
      if (voice) utterance.voice = voice;

      utterance.onstart = function () {
        isSpeaking = true;
        if (listening && recognition) {
          recognition.abort();  // â›”ï¸ stop immediately
        }
      };

      utterance.onend = function () {
        isSpeaking = false;
        if (listening && recognition) {
          setTimeout(() => recognition.start(), 800);  // âœ… add delay
        }
      };

      speechSynthesis.speak(utterance);
    }


    // function speak(text) {
    //   if (!('speechSynthesis' in window)) {
    //     console.warn("This browser does not support speech synthesis.");
    //     return;
    //   }

    //   if (!voicesReady) {
    //     console.warn("Voices not ready yet â€” skipping TTS:", text);
    //     return;
    //   }

    //   const utterance = new SpeechSynthesisUtterance(text);
    //   utterance.lang = 'en-US';

    //   const voices = speechSynthesis.getVoices();
    //   const voice = voices.find(v => v.lang === 'en-US' && v.name.includes("Google"));
    //   if (voice) {
    //     utterance.voice = voice;
    //     console.log("Using voice:", voice.name);
    //   } else {
    //     console.log("No Google US English voice found â€” using default en-US");
    //   }

    //   utterance.onstart = function() {
    //     if (listening && recognition) {
    //       console.log("Pausing recognition during TTS");
    //       recognition.stop();
    //     }
    //   };

    //   utterance.onend = function() {
    //     if (listening && recognition) {
    //       console.log("Resuming recognition after TTS");
    //       recognition.start();
    //     }
    //   };

    //   speechSynthesis.speak(utterance);
    // }

  
    // sendMessage() fires when: 
    // 1) The user types a message (e.g., "what's next?") 
    // 2) the page auto-resumes on load (newPageLoaded=True)
    async function sendMessage(newPageLoaded = false, substepUpdated = false, overrideTranscript = null) {
      console.log("sendMessage called", { newPageLoaded});
      const input = document.getElementById("chat-input");
      const message = newPageLoaded ? "resuming" : (overrideTranscript ? overrideTranscript : input.value.trim()); // Get user input or use overrideMessage if auto is true.
      if (!message && !newPageLoaded) return; // Don't send empty messages unless the page just loaded.
      
      let substep_flags = {};
      if (currentPage === "send_to_alex.html") {
        substep_flags = updateSubstepFlagsForTransferSomeone();
      }
  
      // Append the message to the chat history and display it
      if (!newPageLoaded) {
        appendMessage("user", message);
        chatHistory.push({ role: "user", content: message });
      }

      input.value = "";
  
      const res = await fetch("/tutorbot", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          messages: chatHistory,
          newPageLoaded,
          substepUpdated,
          intent,
          currentPage,
          substep_flags,  // âœ… Send subtask progress
          assistant: "grace",  // Always use Grace for this chatbot
        })
      });
  
      const data = await res.json();
      console.log("data from backend", data)
      // console.log("flags", data.substep_flags);

      intent = data.intent
      selector = data.selector || null;
      botMessage = data.botMessage || "Hmmm not sure if I understand. Please tell me what you want to do.";
      
      appendMessage("assistant", botMessage);
      chatHistory.push({ role: "assistant", content: botMessage });
      sessionStorage.setItem("chatHistory", JSON.stringify(chatHistory));
  
      if (intent && intent !== "unknown") {
        sessionStorage.setItem("intent", intent);
      }
      
      if (selector) {
        sessionStorage.setItem("selector", selector);
        highlight(selector);
      }
    }
  
    // runs only once when chatbot.html is first rendered in the browser (i.e., when the iframe is inserted into the DOM and loads the chatbot page).
    window.addEventListener("DOMContentLoaded", () => {
      // Let parent know which assistant
      console.log("Sending assistant to parent: grace");
      window.parent.postMessage({ instruction: "sendAssistant", assistant: "grace" }, "*");

      // Load chat history, intent
      console.log("DOMContentLoaded");  // ðŸ” baseline check
      chatHistory = JSON.parse(sessionStorage.getItem("chatHistory") || "[]");
      intent = sessionStorage.getItem("intent") || null;

      // Restore listening state to show the listening checkbox and status consistently
      const storedListening = sessionStorage.getItem("listening");
      if (storedListening === "true") {
        listening = true;
        document.getElementById("listen-checkbox").checked = true;
        document.getElementById("listening-status").textContent = "Active";
        if (recognition) recognition.start();
        // âœ… Resume the conversation automatically if listening is active and intent exists
        if (listening && intent) {
          console.log("Auto-resuming on new page");
          sendMessage(newPageLoaded = true);
        }
      } else if (storedListening === "false") {
        listening = false;
        document.getElementById("listen-checkbox").checked = false;
        document.getElementById("listening-status").textContent = "Inactive";
      } else {
        listening = false;
        document.getElementById("listen-checkbox").checked = false;
        document.getElementById("listening-status").textContent = "Inactive";
      }

      // Restore chatbox and history
      for (const m of chatHistory) appendMessage(m.role, m.content, suppressTTS=true); // Re-render chat history
      const input = document.getElementById("chat-input");
      input.addEventListener("keydown", (e) => {
        if (e.key === "Enter") sendMessage();
      });

      // // Automatically send a message: if there's no intent, asking for intent. If there's, resuming the conversation.
      // if (!intent) {
      //   appendMessage("assistant", "Hi! I'm Grace, your banking assistant. How can I help you today?");
      // } else {
      //   // Figure out the step based on intent + page
      //   // This logic is only run when the chatbot is first loaded, not on every message sent.
      //   // This is to ensure that the chatbot can resume the conversation from the correct step.
      //   console.log("calling sendMessage for resuming")
      //   sendMessage(newPageLoaded = true);
      // }

      if (window.parent.location.pathname.endsWith("send_to_alex.html")) {
        const parentDoc = window.parent.document;
        console.log("parentDoc", parentDoc);

        parentDoc.querySelector("#from-account")?.addEventListener("change", () => {
          sendMessage(substepUpdated = true);
        });

        parentDoc.querySelector("#amount")?.addEventListener("change", () => {
          sendMessage(substepUpdated = true);
        });

      }
    });
  
    // This listens for messages from the parent window (the main app) to log user actions.
    window.addEventListener("message", (event) => {
      const { instruction, text } = event.data;
      if (instruction === "log" && typeof text === "string") {
        console.log("Received log message from parent:", text);
        logUserAction(text);
      }
    });
  
    // This saves the chat history, step name, and intent to sessionStorage before the page is unloaded.
    window.addEventListener("beforeunload", () => {
      sessionStorage.setItem("chatHistory", JSON.stringify(chatHistory));
      sessionStorage.setItem("intent", intent);
    });

    // Speech Recognition setup
    let recognition;
    let listening = false;  // global listening flag

    if ('webkitSpeechRecognition' in window) {
      recognition = new webkitSpeechRecognition();  // Chrome
    } else if ('SpeechRecognition' in window) {
      recognition = new SpeechRecognition();  // Firefox
    }

    if (recognition) {
      recognition.continuous = false;  // Not enable continuous listening
      recognition.interimResults = false;
      recognition.lang = 'en-US';

      recognition.onresult = function(event) {
        if (isSpeaking) {
          console.warn("Ignoring recognition during TTS");
          return;
        }
        const transcript = event.results[0][0].transcript;
        console.log("You said:", transcript);
        sendMessage(false, false, transcript);
      };

      recognition.onerror = function(event) {
        if (event.error === "no-speech" || event.error === "aborted") {
          // Suppress benign errors
          console.log("Ignored speech error:", event.error);
          return;
        }

        // Only alert on real unexpected errors
        alert("Speech recognition error: " + event.error);
      };

      recognition.onend = function() {
        if (listening) {
          if (isSpeaking) {
            console.log("TTS still speaking, delaying recognition restart...");
            setTimeout(() => recognition.onend(), 500); // retry until TTS ends
          } else {
            console.log("Restarting recognition...");
            recognition.start();
          }
        }
      };
    } else {
      function toggleListening() {
        alert("Speech recognition is not supported in this browser.");
      }
    }

    window.addEventListener("message", (event) => {
      if (event.data.instruction === "requestAssistant") {
        const assistant = "grace"; // or make dynamic later
        sessionStorage.setItem("assistant", assistant); // For next page
        console.log("Requesting assistant from parent window â€” sending:", assistant);
        window.parent.postMessage({ instruction: "sendAssistant", assistant }, "*");
      }
    });
  </script>
</body>
</html>
